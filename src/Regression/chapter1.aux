\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Linear Regression}{3}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{intro}{{1}{3}}[None]
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Linear Models}{3}{}\protected@file@percent }
\newlabel{sec:1}{{1.1}{3}}[None]
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Univariate Linear Models}{4}{}\protected@file@percent }
\newlabel{sec:2}{{1.2}{4}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.1.py}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Data generated from the function $f(x)=x^2+2\cdot x+1$ with added noise.}}{4}{}\protected@file@percent }
\newlabel{fig:linear1}{{1.1}{4}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.2.py}{5}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.3.py}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Gradient Descent for Univariate Linear Models}{5}{}\protected@file@percent }
\newlabel{subsec:2}{{1.2.1}{5}}[None]
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Initial model prediction with random weight and bias.}}{6}{}\protected@file@percent }
\newlabel{fig:linear2}{{1.2}{6}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.5.py}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1.1}Gradients}{7}{}\protected@file@percent }
\newlabel{eq:gradient_w}{{1.1}{7}}[None]
\newlabel{eq:gradient_b}{{1.2}{7}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.6.py}{8}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.7.py}{8}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Contour plot of the loss function, gradient field, and gradient descent path.}}{9}{}\protected@file@percent }
\newlabel{fig:linear3}{{1.3}{9}}[None]
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1.2}Gradient Descent \& Backward Propagation}{9}{}\protected@file@percent }
\newlabel{subsubsec:2}{{1.2.1.2}{9}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.8.py}{10}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Contour plot of the loss function, gradient field, and gradient descent path.}}{12}{}\protected@file@percent }
\newlabel{fig:linear4}{{1.4}{12}}[None]
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Gradient descent path on the loss function surface. The code used to generate this visualization can be found in \ref {sec:gradient-descent-3d} of the Appendix.}}{13}{}\protected@file@percent }
\newlabel{fig:linear5}{{1.5}{13}}[None]
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Initial and final linear models compared to the true line of best fit.}}{13}{}\protected@file@percent }
\newlabel{fig:linear6}{{1.6}{13}}[None]
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Univariate Linear Models with Hidden Layers}{14}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Circle classification problem in 2D.}}{14}{}\protected@file@percent }
\newlabel{fig:linear7}{{1.7}{14}}[None]
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Circle classification problem in 3D.}}{14}{}\protected@file@percent }
\newlabel{fig:linear8}{{1.8}{14}}[None]
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces The sigmoid activation function and its derivative.}}{16}{}\protected@file@percent }
\newlabel{fig:linear9}{{1.9}{16}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.10.py}{17}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Univariate linear model with a single hidden layer of dimension $d=2$. The initial model's state is shown, as well as its final state after training.}}{18}{}\protected@file@percent }
\newlabel{fig:linear10}{{1.10}{18}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.11.py}{19}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Univariate linear model with a single hidden layer of dimension $d=10$. The initial model's state is shown, as well as its final state after training.}}{20}{}\protected@file@percent }
\newlabel{fig:linear11}{{1.11}{20}}[None]
\@setckpt{Regression/chapter1}{
\setcounter{page}{22}
\setcounter{equation}{2}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{1}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{11}
\setcounter{table}{0}
\setcounter{chapter}{1}
\setcounter{theorem}{0}
\setcounter{case}{0}
\setcounter{conjecture}{0}
\setcounter{corollary}{0}
\setcounter{definition}{0}
\setcounter{example}{0}
\setcounter{exercise}{0}
\setcounter{lemma}{0}
\setcounter{note}{0}
\setcounter{problem}{0}
\setcounter{property}{0}
\setcounter{proposition}{0}
\setcounter{question}{0}
\setcounter{solution}{0}
\setcounter{remark}{0}
\setcounter{prob}{0}
\setcounter{merk}{0}
\setcounter{endNonectr}{10}
\setcounter{currNonectr}{0}
\setcounter{currproofctr}{0}
\setcounter{endproofctr}{0}
\setcounter{proof}{0}
\setcounter{minitocdepth}{0}
\setcounter{@inst}{0}
\setcounter{@auth}{0}
\setcounter{auco}{0}
\setcounter{contribution}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{81}
\setcounter{float@type}{8}
\setcounter{lstlisting}{0}
}
