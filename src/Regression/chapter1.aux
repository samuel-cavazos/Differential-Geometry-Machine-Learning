\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Linear Regression}{3}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{intro}{{1}{3}}[None]
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Linear Models}{3}{}\protected@file@percent }
\newlabel{sec:1}{{1.1}{3}}[None]
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Univariate Linear Models}{4}{}\protected@file@percent }
\newlabel{sec:2}{{1.2}{4}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.1.py}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Data generated from the function $f(x)=x^2+2\cdot x+1$ with added noise.}}{5}{}\protected@file@percent }
\newlabel{fig:linear1}{{1.1}{5}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.2.py}{5}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.3.py}{5}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Initial model prediction with random weight and bias.}}{6}{}\protected@file@percent }
\newlabel{fig:linear2}{{1.2}{6}}[None]
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Gradient Descent for Univariate Linear Models}{6}{}\protected@file@percent }
\newlabel{subsec:2}{{1.2.1}{6}}[None]
\newlabel{eq:mse}{{1.1}{6}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.5.py}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1.1}Gradients}{7}{}\protected@file@percent }
\newlabel{eq:gradient-w}{{1.2}{8}}[None]
\newlabel{eq:gradient-w-matrix}{{1.3}{8}}[None]
\newlabel{eq:gradient-b}{{1.4}{8}}[None]
\newlabel{eq:gradient-b-matrix}{{1.5}{8}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.6.py}{9}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.7.py}{9}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Contour plot of the loss function, gradient field, and gradient descent path.}}{10}{}\protected@file@percent }
\newlabel{fig:linear3}{{1.3}{10}}[None]
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1.2}Gradient Descent \& Backward Propagation}{10}{}\protected@file@percent }
\newlabel{subsubsec:2}{{1.2.1.2}{10}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.8.py}{11}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Contour plot of the loss function, gradient field, and gradient descent path.}}{13}{}\protected@file@percent }
\newlabel{fig:linear4}{{1.4}{13}}[None]
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Gradient descent path on the loss function surface. The code used to generate this visualization can be found in \ref {sec:gradient-descent-3d} of the Appendix.}}{14}{}\protected@file@percent }
\newlabel{fig:linear5}{{1.5}{14}}[None]
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Initial and final linear models compared to the true line of best fit.}}{14}{}\protected@file@percent }
\newlabel{fig:linear6}{{1.6}{14}}[None]
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Univariate Linear Models with Hidden Layers}{15}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Circle classification problem in 2D.}}{15}{}\protected@file@percent }
\newlabel{fig:linear7}{{1.7}{15}}[None]
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Circle classification problem in 3D.}}{15}{}\protected@file@percent }
\newlabel{fig:linear8}{{1.8}{15}}[None]
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces The sigmoid activation function and its derivative.}}{18}{}\protected@file@percent }
\newlabel{fig:linear9}{{1.9}{18}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.10.py}{18}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Univariate linear model with a single hidden layer of dimension $d=2$. The initial model's state is shown, as well as its final state after training.}}{20}{}\protected@file@percent }
\newlabel{fig:linear10}{{1.10}{20}}[None]
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.1.1.11.py}{20}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces Univariate linear model with a single hidden layer of dimension $d=10$. The initial model's state is shown, as well as its final state after training.}}{22}{}\protected@file@percent }
\newlabel{fig:linear11}{{1.11}{22}}[None]
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Multivariate Linear Models}{23}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Sample data from \texttt  {seaborn}'s \emph  {Tips} dataset}}{23}{}\protected@file@percent }
\newlabel{tab:sample_data}{{1.1}{23}}[None]
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Encoding Categorical Data}{24}{}\protected@file@percent }
\newlabel{subsec:3}{{1.3.1}{24}}[None]
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1.1}Label Encoding Categorical Data}{24}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.3.1.1.py}{24}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1.2}{Ordinal Encoding Categorical Data}}{25}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.3.1.3.py}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1.3}One-Hot Encoding Categorical Data}{25}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.3.1.2.py}{26}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{Regression/code/1.3.1.4.py}{26}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces Random sample of the encoded \emph  {Tips} dataset}}{28}{}\protected@file@percent }
\newlabel{tab:encoded_data}{{1.2}{28}}[None]
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1.4}The Jacobian Matrix}{29}{}\protected@file@percent }
\newlabel{eq:jacobian}{{1.6}{29}}[None]
\@writefile{thm}{\contentsline {proof}{{Proof}{1}{}}{30}{}\protected@file@percent }
\global\def\markiproofi{\ensuremath {\square }}
\@setckpt{Regression/chapter1}{
\setcounter{page}{31}
\setcounter{equation}{6}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{1}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{subsubsection}{4}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{11}
\setcounter{table}{2}
\setcounter{chapter}{1}
\setcounter{theorem}{0}
\setcounter{case}{0}
\setcounter{conjecture}{0}
\setcounter{corollary}{0}
\setcounter{definition}{1}
\setcounter{example}{2}
\setcounter{exercise}{0}
\setcounter{lemma}{0}
\setcounter{note}{0}
\setcounter{problem}{0}
\setcounter{property}{0}
\setcounter{proposition}{0}
\setcounter{question}{0}
\setcounter{solution}{0}
\setcounter{remark}{0}
\setcounter{prob}{0}
\setcounter{merk}{0}
\setcounter{endNonectr}{15}
\setcounter{currNonectr}{0}
\setcounter{currproofctr}{1}
\setcounter{endproofctr}{1}
\setcounter{proof}{1}
\setcounter{minitocdepth}{0}
\setcounter{@inst}{0}
\setcounter{@auth}{0}
\setcounter{auco}{0}
\setcounter{contribution}{0}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{14}
\setcounter{float@type}{8}
\setcounter{lstlisting}{0}
}
